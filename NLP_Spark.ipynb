{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Spark and Python for Big Data Final Exam-Branches.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa486f23"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "id": "fa486f23"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abbf1e33"
      },
      "source": [
        "##### **Good luck with taking your exam. Keep working and make your dreams all come true. Seeing the results of all of your hard work will make this struggle worth it. We’re all thinking of you.** \n",
        "<b><font color='blue'>AI-PRO Spark Team ITI</font></b>"
      ],
      "id": "abbf1e33"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9da32d6"
      },
      "source": [
        "# NLP Using PySpark"
      ],
      "id": "c9da32d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8326ba88"
      },
      "source": [
        "## Objective:\n",
        "- The objective from this project is to create a <b>Spam filter using NaiveBayes classifier</b>.\n",
        "- It is required to obtain <b>f1_scored > 0.9</b>.\n",
        "- We'll use a dataset from UCI Repository. SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "- Data is also provided for you in the assignment (you do not have to download it)."
      ],
      "id": "8326ba88"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6971f788"
      },
      "source": [
        "## To perform this task follow the following guiding steps:"
      ],
      "id": "6971f788"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e31bc851"
      },
      "source": [
        "### Create a spark session and import the required libraries"
      ],
      "id": "e31bc851"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcf86e46"
      },
      "source": [
        "# importing libraries\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Final').getOrCreate()"
      ],
      "id": "dcf86e46",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90c7df9e"
      },
      "source": [
        "### Read the readme file to learn more about the data"
      ],
      "id": "90c7df9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d00718f"
      },
      "source": [
        "### Read the data into a DataFrame"
      ],
      "id": "2d00718f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29914cf1"
      },
      "source": [
        "path=\"/content/SMSSpamCollection\"\n",
        "df = spark.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"inferSchema\",True).load(path)"
      ],
      "id": "29914cf1",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWENnSgqZ6Qm",
        "outputId": "32e98652-2167-4663-9e1c-1d2ec776d07e"
      },
      "source": [
        "df.show()"
      ],
      "id": "jWENnSgqZ6Qm",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "| _c0|                 _c1|\n",
            "+----+--------------------+\n",
            "| ham|Go until jurong p...|\n",
            "| ham|Ok lar... Joking ...|\n",
            "|spam|Free entry in 2 a...|\n",
            "| ham|U dun say so earl...|\n",
            "| ham|Nah I don't think...|\n",
            "|spam|FreeMsg Hey there...|\n",
            "| ham|Even my brother i...|\n",
            "| ham|As per your reque...|\n",
            "|spam|WINNER!! As a val...|\n",
            "|spam|Had your mobile 1...|\n",
            "| ham|I'm gonna be home...|\n",
            "|spam|SIX chances to wi...|\n",
            "|spam|URGENT! You have ...|\n",
            "| ham|I've been searchi...|\n",
            "| ham|I HAVE A DATE ON ...|\n",
            "|spam|XXXMobileMovieClu...|\n",
            "| ham|Oh k...i'm watchi...|\n",
            "| ham|Eh u remember how...|\n",
            "| ham|Fine if thats th...|\n",
            "|spam|England v Macedon...|\n",
            "+----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "182cd7f6"
      },
      "source": [
        "### Print the schema"
      ],
      "id": "182cd7f6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52706b9",
        "outputId": "0bdefd2d-8f8e-4242-db1e-a7ee3a04ac46"
      },
      "source": [
        "df.printSchema()"
      ],
      "id": "b52706b9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b90cce7"
      },
      "source": [
        "### Rename the first column to 'class' and second column to 'text'"
      ],
      "id": "2b90cce7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1a88df6",
        "outputId": "ede67150-5cdc-4e9d-adbd-172a19416245"
      },
      "source": [
        "df=df.withColumnRenamed(\"_c0\", 'class')\n",
        "df=df.withColumnRenamed(\"_c1\", 'text')\n",
        "df.printSchema()"
      ],
      "id": "f1a88df6",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62a544fc"
      },
      "source": [
        ""
      ],
      "id": "62a544fc",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e798d0"
      },
      "source": [
        "### Show the first 10 rows from the dataframe\n",
        "- Show once with truncate=True and once with truncate=False"
      ],
      "id": "a3e798d0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "362dcb99",
        "outputId": "b079de99-88b9-4409-ef5b-a8bd741f2472"
      },
      "source": [
        "df.show(10,truncate=False)"
      ],
      "id": "362dcb99",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|class|text                                                                                                                                                            |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|ham  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                 |\n",
            "|ham  |Ok lar... Joking wif u oni...                                                                                                                                   |\n",
            "|spam |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's     |\n",
            "|ham  |U dun say so early hor... U c already then say...                                                                                                               |\n",
            "|ham  |Nah I don't think he goes to usf, he lives around here though                                                                                                   |\n",
            "|spam |FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv             |\n",
            "|ham  |Even my brother is not like to speak with me. They treat me like aids patent.                                                                                   |\n",
            "|ham  |As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune|\n",
            "|spam |WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.   |\n",
            "|spam |Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030      |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fe744a9"
      },
      "source": [
        "## Clean and Prepare the Data"
      ],
      "id": "2fe744a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d693167"
      },
      "source": [
        "### Create a new feature column contains the length of the text column"
      ],
      "id": "4d693167"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5424a0cb"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "df= df.withColumn('length',length(df.text))"
      ],
      "id": "5424a0cb",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ea2de6"
      },
      "source": [
        "### Show the new dataframe"
      ],
      "id": "78ea2de6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04c67c53",
        "outputId": "36acfea1-fd3a-4653-9a06-262da91c5956"
      },
      "source": [
        "df.show()"
      ],
      "id": "04c67c53",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+\n",
            "|class|                text|length|\n",
            "+-----+--------------------+------+\n",
            "|  ham|Go until jurong p...|   111|\n",
            "|  ham|Ok lar... Joking ...|    29|\n",
            "| spam|Free entry in 2 a...|   155|\n",
            "|  ham|U dun say so earl...|    49|\n",
            "|  ham|Nah I don't think...|    61|\n",
            "| spam|FreeMsg Hey there...|   147|\n",
            "|  ham|Even my brother i...|    77|\n",
            "|  ham|As per your reque...|   160|\n",
            "| spam|WINNER!! As a val...|   157|\n",
            "| spam|Had your mobile 1...|   154|\n",
            "|  ham|I'm gonna be home...|   109|\n",
            "| spam|SIX chances to wi...|   136|\n",
            "| spam|URGENT! You have ...|   155|\n",
            "|  ham|I've been searchi...|   196|\n",
            "|  ham|I HAVE A DATE ON ...|    35|\n",
            "| spam|XXXMobileMovieClu...|   149|\n",
            "|  ham|Oh k...i'm watchi...|    26|\n",
            "|  ham|Eh u remember how...|    81|\n",
            "|  ham|Fine if thats th...|    56|\n",
            "| spam|England v Macedon...|   155|\n",
            "+-----+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692e37a6"
      },
      "source": [
        "### Get the average text length for each class (give alias name to the average length column)"
      ],
      "id": "692e37a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWT1aF1_d6hm",
        "outputId": "b0de62ac-6aa0-493b-cf15-57f15f61e3e1"
      },
      "source": [
        "df.groupby(\"class\").agg(avg(\"length\").alias(\"the average length\")).show()"
      ],
      "id": "eWT1aF1_d6hm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|class|the average length|\n",
            "+-----+------------------+\n",
            "|  ham| 71.45431945307645|\n",
            "| spam| 138.6706827309237|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5e101af"
      },
      "source": [
        "## Feature Transformations"
      ],
      "id": "d5e101af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "838ad9dd"
      },
      "source": [
        "### In this part you transform you raw text in to tf_idf model :\n",
        "- For more information about TF-IDF check the following link: <b>(Not needed for the test)</b>\n",
        "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
      ],
      "id": "838ad9dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "225067d5"
      },
      "source": [
        "### Perform the following steps to obtain TF-IDF:\n",
        "1. Import the required transformers/estimators for the subsequent steps.\n",
        "2. Create a <b>Tokenizer</b> from the text column.\n",
        "3. Create a <b>StopWordsRemover</b> to remove the <b>stop words</b> from the column obtained from the <b>Tokenizer</b>.\n",
        "4. Create a <b>CountVectorizer</b> after removing the <b>stop words</b>.\n",
        "5. Create the <b>TF-IDF</b> from the <b>CountVectorizer</b>."
      ],
      "id": "225067d5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a4eebf8"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector"
      ],
      "id": "6a4eebf8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b82756db"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokenized_text\")\n",
        "stop_remove = StopWordsRemover(inputCol='tokenized_text',outputCol='stop_tokens')\n",
        "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
        "tf_idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")"
      ],
      "id": "b82756db",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1527ad65"
      },
      "source": [
        "- Convert the <b>class column</b> to index using <b>StringIndexer</b>\n",
        "- Create feature column from the <b>TF-IDF</b> and <b>lenght</b> columns."
      ],
      "id": "1527ad65"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaf46159"
      },
      "source": [
        "ham_spam_numeric = StringIndexer(inputCol='class',outputCol='label')\n",
        "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
      ],
      "id": "aaf46159",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad9d4c52"
      },
      "source": [
        ""
      ],
      "id": "ad9d4c52",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9775d494"
      },
      "source": [
        "## The Model\n",
        "- Create a <b>NaiveBayes</b> classifier with the default parameters."
      ],
      "id": "9775d494"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57af0d5d"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "NaiveBayes_model = NaiveBayes()"
      ],
      "id": "57af0d5d",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54c7384e"
      },
      "source": [
        ""
      ],
      "id": "54c7384e",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc14de63"
      },
      "source": [
        "## Pipeline\n",
        "### Create a pipeline model contains all the steps starting from the Tokenizer to the NaiveBays classifier."
      ],
      "id": "dc14de63"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ee0d1ca"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "df_pipe_NaiveBayes = Pipeline(stages=[tokenizer,stop_remove,count_vec,tf_idf,ham_spam_numeric,clean_up,NaiveBayes_model])\n"
      ],
      "id": "8ee0d1ca",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5d7efbe"
      },
      "source": [
        "### Split your data to trian and test data with ratios 0.7 and 0.3 respectively."
      ],
      "id": "f5d7efbe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2843d997"
      },
      "source": [
        "(training,testing) = df.randomSplit([0.7,0.3],seed=42)\n"
      ],
      "id": "2843d997",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bcea576"
      },
      "source": [
        "### Fit your Pipeline model to the training data"
      ],
      "id": "8bcea576"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c5d4681"
      },
      "source": [
        "spam_predictor = df_pipe_NaiveBayes.fit(training)"
      ],
      "id": "3c5d4681",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228a3eb1"
      },
      "source": [
        "### Perform predictions on tests dataframe"
      ],
      "id": "228a3eb1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14f4aab5",
        "outputId": "6e90c36e-27d1-4206-dd7d-acd2ab94a85c"
      },
      "source": [
        "test_results = spam_predictor.transform(testing)\n",
        "test_results.show()"
      ],
      "id": "14f4aab5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|class|                text|length|      tokenized_text|         stop_tokens|               c_vec|              tf_idf|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  ham| &lt;DECIMAL&gt; ...|   132|[, &lt;decimal&gt...|[, &lt;decimal&gt...|(11001,[3,81,128,...|(11001,[3,81,128,...|  0.0|(11002,[3,81,128,...|[-688.90165603252...|[1.0,1.6326453475...|       0.0|\n",
            "|  ham| said kiss, kiss,...|   133|[, said, kiss,, k...|[, said, kiss,, k...|(11001,[3,115,200...|(11001,[3,115,200...|  0.0|(11002,[3,115,200...|[-654.35621509625...|[1.0,1.4746946214...|       0.0|\n",
            "|  ham| what number do u...|    36|[, what, number, ...|[, number, u, liv...|(11001,[0,3,86,19...|(11001,[0,3,86,19...|  0.0|(11002,[0,3,86,19...|[-215.15028157365...|[0.99999999423701...|       0.0|\n",
            "|  ham|\"Gimme a few\" was...|    41|[\"gimme, a, few\",...|[\"gimme, few\", , ...|(11001,[3,8,302,1...|(11001,[3,8,302,1...|  0.0|(11002,[3,8,302,1...|[-189.10211574399...|[1.0,3.8861314826...|       0.0|\n",
            "|  ham|\"Response\" is one...|   154|[\"response\", is, ...|[\"response\", one,...|(11001,[2,7,22,55...|(11001,[2,7,22,55...|  0.0|(11002,[2,7,22,55...|[-745.29687938575...|[1.0,1.6458130236...|       0.0|\n",
            "|  ham|\"SYMPTOMS\" when U...|   139|[\"symptoms\", when...|[\"symptoms\", u, l...|(11001,[0,5,10,33...|(11001,[0,5,10,33...|  0.0|(11002,[0,5,10,33...|[-587.67666990702...|[1.0,8.0680382491...|       0.0|\n",
            "|  ham|\"Speak only when ...|    80|[\"speak, only, wh...|[\"speak, feel, wo...|(11001,[97,122,24...|(11001,[97,122,24...|  0.0|(11002,[97,122,24...|[-209.58014676693...|[1.0,1.3544781423...|       0.0|\n",
            "|  ham|&lt;#&gt;  great ...|    85|[&lt;#&gt;, , gre...|[&lt;#&gt;, , gre...|(11001,[3,8,33,68...|(11001,[3,8,33,68...|  0.0|(11002,[3,8,33,68...|[-683.73323971710...|[1.0,7.1156160375...|       0.0|\n",
            "|  ham|&lt;#&gt;  w jett...|    37|[&lt;#&gt;, , w, ...|[&lt;#&gt;, , w, ...|(11001,[3,8,294,6...|(11001,[3,8,294,6...|  0.0|(11002,[3,8,294,6...|[-223.51130804081...|[1.0,1.2761668297...|       0.0|\n",
            "|  ham|&lt;#&gt; , that'...|    48|[&lt;#&gt;, ,, th...|[&lt;#&gt;, ,, al...|(11001,[8,216,249...|(11001,[8,216,249...|  0.0|(11002,[8,216,249...|[-318.30932270534...|[1.0,8.9166965788...|       0.0|\n",
            "|  ham|&lt;#&gt; ISH MIN...|    45|[&lt;#&gt;, ish, ...|[&lt;#&gt;, ish, ...|(11001,[8,272,302...|(11001,[8,272,302...|  0.0|(11002,[8,272,302...|[-427.70624973278...|[1.0,2.3859823204...|       0.0|\n",
            "|  ham|(I should add tha...|   132|[(i, should, add,...|[(i, add, really,...|(11001,[5,17,76,1...|(11001,[5,17,76,1...|  0.0|(11002,[5,17,76,1...|[-499.84278582921...|[1.0,3.2926095426...|       0.0|\n",
            "|  ham|(No promises on w...|    60|[(no, promises, o...|[(no, promises, t...|(11001,[106,382,3...|(11001,[106,382,3...|  0.0|(11002,[106,382,3...|[-347.19047894351...|[1.0,4.9674012636...|       0.0|\n",
            "|  ham|(That said can yo...|    43|[(that, said, can...|[(that, said, tex...|(11001,[19,22,115...|(11001,[19,22,115...|  0.0|(11002,[19,22,115...|[-111.46806031876...|[0.99999999999059...|       0.0|\n",
            "|  ham|* Am on a train b...|    56|[*, am, on, a, tr...|[*, train, back, ...|(11001,[44,213,11...|(11001,[44,213,11...|  0.0|(11002,[44,213,11...|[-403.28608032272...|[1.0,1.4624641481...|       0.0|\n",
            "|  ham|* Thought I didn'...|    27|[*, thought, i, d...|[*, thought, see,...|(11001,[33,67,153...|(11001,[33,67,153...|  0.0|(11002,[33,67,153...|[-152.64563009307...|[0.99999999999999...|       0.0|\n",
            "|  ham|* Was a nice day ...|   140|[*, was, a, nice,...|[*, nice, day, an...|(11001,[43,45,122...|(11001,[43,45,122...|  0.0|(11002,[43,45,122...|[-465.83889066944...|[1.0,2.2000394729...|       0.0|\n",
            "|  ham|* Will have two m...|    67|[*, will, have, t...|[*, two, cartons,...|(11001,[0,213,369...|(11001,[0,213,369...|  0.0|(11002,[0,213,369...|[-203.62640637654...|[0.99999999999794...|       0.0|\n",
            "|  ham|, ,  and  picking...|   169|[,, ,, , and, , p...|[,, ,, , , pickin...|(11001,[0,2,3,7,2...|(11001,[0,2,3,7,2...|  0.0|(11002,[0,2,3,7,2...|[-1296.7649568016...|[1.0,7.1750137313...|       0.0|\n",
            "|  ham|, how's things? J...|    38|[,, how's, things...|[,, things?, quic...|(11001,[249,932,3...|(11001,[249,932,3...|  0.0|(11002,[249,932,3...|[-276.17849722491...|[1.0,2.1414904430...|       0.0|\n",
            "+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce2885f"
      },
      "source": [
        "### Print the schema of the prediction dataframe"
      ],
      "id": "bce2885f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKUoNgdaiBJr",
        "outputId": "978c4642-1dc2-4c66-e235-92c1bcd6ddd9"
      },
      "source": [
        "test_results.printSchema()"
      ],
      "id": "PKUoNgdaiBJr",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- length: integer (nullable = true)\n",
            " |-- tokenized_text: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- stop_tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- c_vec: vector (nullable = true)\n",
            " |-- tf_idf: vector (nullable = true)\n",
            " |-- label: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e3ea341"
      },
      "source": [
        ""
      ],
      "id": "6e3ea341",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57f27055"
      },
      "source": [
        "## Model Evaluation\n",
        "- Use <b>MulticlassClassificationEvaluator</b> to calculate the <b>f1_score</b>."
      ],
      "id": "57f27055"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61911086",
        "outputId": "024d3d15-4257-4636-e60b-2e6e7378ae67"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "f1_eval = MulticlassClassificationEvaluator(metricName='f1')\n",
        "f1 = f1_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting spam was: {}\".format(f1))"
      ],
      "id": "61911086",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model at predicting spam was: 0.9727502290227267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOftJiDWoKue"
      },
      "source": [
        "#we will use another model to compare results with each other"
      ],
      "id": "mOftJiDWoKue"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-B2Bef9ob-A"
      },
      "source": [
        "##we will logistic regression"
      ],
      "id": "0-B2Bef9ob-A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be706565"
      },
      "source": [
        "# Import LogisticRegression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create a LogisticRegression Estimator\n",
        "lr = LogisticRegression()"
      ],
      "id": "be706565",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mYrfW-Oovvm"
      },
      "source": [
        "##Our metric will be areaUnderROC"
      ],
      "id": "7mYrfW-Oovvm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1f9ba1"
      },
      "source": [
        "# Import the evaluation submodule\n",
        "import pyspark.ml.evaluation as evals\n",
        "\n",
        "# Create a BinaryClassificationEvaluator\n",
        "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
      ],
      "id": "af1f9ba1",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mc2-1jspA4S"
      },
      "source": [
        "##Next, you need to create a grid of values to search over when looking for the optimal hyperparameters"
      ],
      "id": "4mc2-1jspA4S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFb-vFkro8p3"
      },
      "source": [
        "# Import the tuning submodule\n",
        "import pyspark.ml.tuning as tune\n",
        "import numpy as np\n",
        "# Create the parameter grid\n",
        "grid = tune.ParamGridBuilder()\n",
        "\n",
        "# Add the hyperparameter\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
        "\n",
        "# Build the grid\n",
        "grid = grid.build()"
      ],
      "id": "BFb-vFkro8p3",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iguSdTmqo83V"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "df_pipe_logistic = Pipeline(stages=[tokenizer,stop_remove,count_vec,tf_idf,ham_spam_numeric,clean_up,lr])\n",
        "\n",
        "# Call data_prep_pipe_logistic.fit()\n",
        "best_lr = df_pipe_logistic.fit(training)\n",
        "\n"
      ],
      "id": "iguSdTmqo83V",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4PYGDSKo8-x",
        "outputId": "f0431356-53f8-471c-86c8-761934688992"
      },
      "source": [
        "# Use the model to predict the test set\n",
        "test_results = best_lr.transform(testing)\n",
        "\n",
        "# Evaluate the predictions\n",
        "print(evaluator.evaluate(test_results))"
      ],
      "id": "n4PYGDSKo8-x",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9860881505061878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ShqqHGzrmZB"
      },
      "source": [
        "y_true = test_results.select(['label']).collect()\n",
        "y_pred = test_results.select(['prediction']).collect()"
      ],
      "id": "2ShqqHGzrmZB",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxY-g6_1rwUL",
        "outputId": "5e6084eb-d585-453a-994d-b45866dc0102"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "id": "KxY-g6_1rwUL",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.99      1381\n",
            "         1.0       0.96      0.89      0.92       212\n",
            "\n",
            "    accuracy                           0.98      1593\n",
            "   macro avg       0.97      0.94      0.96      1593\n",
            "weighted avg       0.98      0.98      0.98      1593\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2vl0CFKrESS"
      },
      "source": [
        "#The conclusion that logistic regression give us area under ROC curve =0.98\n",
        "#which is so good as it close to 1 "
      ],
      "id": "w2vl0CFKrESS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00e7b53"
      },
      "source": [
        "# GOOD LUCK\n",
        "<b><font color='GREEN'>AI-PRO Spark Team ITI</font></b>"
      ],
      "id": "e00e7b53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e065922"
      },
      "source": [
        "![image-3.png](attachment:image-3.png)"
      ],
      "id": "0e065922"
    }
  ]
}